import numpy as np
from abc import ABC, abstractmethod
import tensorflow as tf
# layer for CNN
from tensorflow.keras.layers import Embedding, Attention, Concatenate, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Input, Dropout, Dense, Average
# layer for LSTM
from tensorflow.keras.layers import LSTM, Bidirectional
# layer for Transformer
from tensorflow.keras import layers
from tensorflow import keras
from tqdm import tqdm
from tensorflow.keras import metrics


# code adapted from https://keras.io/examples/nlp/text_classification_with_transformer/
class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim = 131, num_heads = 128, ff_dim = 512, rate=0.1, name=None,**kwargs):
        super(TransformerBlock, self).__init__(name=name)
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)
    
    
    def get_config(self):
        config = super(TransformerBlock, self).get_config()
        config.update({"att": self.att})
        config.update({"ffn": self.ffn})
        config.update({"layernorm1": self.layernorm1})
        config.update({"layernorm2": self.layernorm2})
        config.update({"dropout1": self.dropout1})
        config.update({"dropout2": self.dropout2})
        return config
    

    def call(self, inputs, training):
        attn_output, attn_output_raw = self.att(inputs, inputs, return_attention_scores=True)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output), attn_output_raw

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim,**kwargs):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)
    
    
    def get_config(self):
        config = super(TokenAndPositionEmbedding, self).get_config()
        config.update({"token_emb": self.token_emb})
        config.update({"pos_emb": self.pos_emb})
        return config
    
    
    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions
        
        
class TokenAndPositionEmbeddingMultipleInputs(layers.Layer):
    def __init__(self, maxlen = 21, vocab_sizes = [261], embed_dims = [128], num_numerical = 3, embed_shapes = [10], name=None,**kwargs):
        super(TokenAndPositionEmbeddingMultipleInputs, self).__init__(name=name)
        self.token_embs = []
        for i in range(len(vocab_sizes)):
            self.token_embs.append(layers.Embedding(input_dim=vocab_sizes[i], output_dim=embed_dims[i]))
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=np.sum(embed_dims) + num_numerical)
        self.concat = layers.Concatenate()
        if embed_shapes is None:
            self.embed_shapes = [1 for i in range(len(embed_dims))]
        else:
            self.embed_shapes = embed_shapes
    
    
    def get_config(self):
        config = super(TokenAndPositionEmbeddingMultipleInputs, self).get_config()
        config.update({"token_embs": self.token_embs})
        config.update({"pos_emb": self.pos_emb})
        config.update({"concat": self.concat})
        config.update({"embed_shapes": self.embed_shapes})
        return config
    
    
    def call(self, embedding_features,x2):
        maxlen = tf.shape(x2)[1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        feature_list = []
        for i in range(len(embedding_features)):
            if self.embed_shapes[i] == 1:
                feature_list.append(self.token_embs[i](embedding_features[i]))
            else:
                # create embedding for all dimensions and then average
                cur_embedding_list = []
                for j in range(self.embed_shapes[i]):
                    cur_embedding_list.append(self.token_embs[i](embedding_features[i][:,:,j]))
                average_embeddings = Average()(cur_embedding_list)
                feature_list.append(average_embeddings)
        feature_list.append(x2)
        concat_embedding = self.concat(feature_list)
        return concat_embedding + positions
        

class NNLanguageModel(ABC):
    def __init__(self, name = 'base_model'):
        self.name = name

    @abstractmethod
    def fit(self, data, **kwargs):
        self._model['trained'] = True
        # fit the model

    @abstractmethod
    def get_model_architecture(self, **kwargs):
        return
        # return model

    def save(self, model_path):
        self.model.save(model_path)
        if model_path.endswith('/'):
            model_path = model_path[0:-1]
        self.model.save_weights(model_path + '.h5', overwrite=True)

    def load(self, model_path):
        if model_path.endswith('.h5'):
            self.model.load_weights(model_path)
        else:
            try:
                self.model = tf.keras.models.load_model(model_path, 
                            custom_objects={'TokenAndPositionEmbeddingMultipleInputs': TokenAndPositionEmbeddingMultipleInputs,
                                'TokenAndPositionEmbedding':TokenAndPositionEmbedding,
                                'TransformerBlock':TransformerBlock})
            except:
                self.model.load_weights(model_path + '/variables/variables')
       
        
    def load_weights_from_model(self, input_model):
        if type(input_model) == TransformerLanguageModel or type(input_model) == CNNLanguageModel or type(input_model) == LSTMLanguageModel:
            input_model = input_model.model
        m2_layers = input_model.layers
        m1_layers = self.model.layers
        for i in range(len(m2_layers)):
            cur_m2_layer = m2_layers[i]
            cur_m1_layer = m1_layers[i]
            if cur_m2_layer.output_shape == cur_m1_layer.output_shape:
                if cur_m2_layer.weights != []:
                    self.model.layers[i].set_weights(input_model.layers[i].get_weights())
                    print('set weights for layer: ' + self.model.layers[i].name)
    
    def transform_to_finetune_model(self):
        for i in range(len(self.model.layers)):
            self.model.layers[i].trainable = False
        self.model.get_layer('final_dense').trainable = True
        
    def predict(self, test_data, batch_size = 32):
        if len(test_data) > 1:
            pred_len = test_data[0].shape[0]
        step_size = 1000
        
        predictions = []
        num_steps = int(np.ceil(pred_len / step_size))
        for i in tqdm(np.arange(num_steps)):
            cur_data_list = []
            for j in range(len(test_data)):
                cur_data_list.append(test_data[j][i*step_size:(i+1)*step_size,:])
            cur_preds = self.model.predict(cur_data_list, batch_size = batch_size)
            cur_preds = list(np.argmax(cur_preds,axis=1))
            predictions += cur_preds
        return predictions
        
    def predict_proba(self, test_data, batch_size = 32):
        return self.model.predict(test_data, batch_size = batch_size)

    
    def fit(self, train_data,train_label,
            val_data,val_label, early_stoppping_patients = 10,
                batch_size = 128, num_epochs = 1000,
                learning_rate=0.001,
                loss = 'sparse_categorical_crossentropy'):
        from tensorflow.keras.callbacks import EarlyStopping
        early_stopping = EarlyStopping(monitor='loss', patience=early_stoppping_patients)
        
        
        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        self.model.compile(optimizer = opt, loss = loss, 
                        metrics=["accuracy"])
        history = self.model.fit(train_data, train_label, batch_size=batch_size, epochs=num_epochs,
                            validation_data=(val_data, val_label),
                            callbacks = [early_stopping],
                            shuffle=True)
        
        return history.history
    
    
class CNNLanguageModel(NNLanguageModel):
    def __init__(self, vocab_sizes, embed_dims, maxlen,
                 num_numerical,
                 cnn_num_filters, cnn_kernel_size,
                 name='cnn_language_model',
                 num_dense = [1024],
                 vocab_size_label = None,
                 flag_use_attention_before_cnn = True,
                 flag_use_attention_before_dense = True,
                 embedding_aggregation_mode = 'average',
                 embed_shapes = None,
                 last_activation = 'softmax'):
        super().__init__(name)
        
        self.name = name
        self.flag_use_attention_before_cnn = flag_use_attention_before_cnn
        self.vocab_sizes = vocab_sizes
        self.embed_dims = embed_dims
        self.maxlen = maxlen
        self.num_numerical = num_numerical
        self.cnn_num_filters = cnn_num_filters
        self.cnn_kernel_size = cnn_kernel_size
        self.vocab_size_label = vocab_size_label
        self.num_dense = num_dense
        self.embedding_aggregation_mode = embedding_aggregation_mode
        self.last_activation = last_activation
        self.flag_use_attention_before_dense = flag_use_attention_before_dense
        if embed_shapes is None:
            self.embed_shapes = [1 for i in range(len(embed_dims))]
        else:
            self.embed_shapes = embed_shapes
        
        
        self.model = self.build_model()
        
        
    def build_model(self):
        if self.vocab_size_label is None:
            self.vocab_size_label = self.vocab_sizes[0]
    
        embedding_inputs = []
        embedding_list = []
        for i in range(len(self.embed_dims)):    
            if self.embed_shapes[i] == 1:
                embedding_inputs.append(layers.Input(shape=(self.maxlen,)))
                embedding_list.append(Embedding(input_dim = self.vocab_sizes[i], output_dim=self.embed_dims[i],input_length = self.maxlen)(embedding_inputs[-1]))
            else:
                embedding_inputs.append(layers.Input(shape=(self.maxlen,self.embed_shapes[i])))
                cur_embedding = Embedding(input_dim = self.vocab_sizes[i], output_dim=self.embed_dims[i],input_length = self.maxlen)
                cur_embedding_list = []
                for j in range(self.embed_shapes[i]):
                    cur_embedding_list.append(cur_embedding(embedding_inputs[-1][:,:,j]))
                average_embeddings = Average()(cur_embedding_list)
                embedding_list.append(average_embeddings)
        
        inputs_num = layers.Input(shape=(self.maxlen,self.num_numerical))
        if self.embedding_aggregation_mode == 'average':
            if len(embedding_list) > 1:
                before_embedding = Average()(embedding_list)
                before_embedding = Concatenate()([before_embedding, inputs_num])
            else:
                concat_input_list = list(embedding_list) + [inputs_num]
                before_embedding = Concatenate()(concat_input_list)
        elif self.embedding_aggregation_mode == 'concat':
            concat_input_list = list(embedding_list) + [inputs_num]
            
            # concatenate embedding and numerical features
            before_embedding = Concatenate()(concat_input_list)
        
        
                
        # add Attenton after Embedding
        if self.flag_use_attention_before_cnn:
            attention_seq = Attention()([before_embedding, before_embedding])
            before_embedding = Concatenate()([before_embedding, attention_seq])
        
        # add CNN layers
        if type(self.cnn_num_filters) == int:
            encoding = Conv1D(
                filters=self.cnn_num_filters,
                kernel_size=self.cnn_kernel_size,
                # Use 'same' padding so outputs have the same shape as inputs.
                padding='same')(before_embedding)
        else:
            for i in range(len(self.cnn_num_filters)):
                if i == 0:
                    encoding = Conv1D(
                        filters=int(self.cnn_num_filters[i]),
                        kernel_size=int(self.cnn_kernel_size[i]),
                        padding='same')(before_embedding)                    
                else:
                    encoding = Conv1D(
                        filters=int(self.cnn_num_filters[i]),
                        kernel_size=int(self.cnn_kernel_size[i]),
                        padding='same')(encoding)                    
                encoding = MaxPooling1D(padding='same')(encoding)
        # add Attention after CNN
        if self.flag_use_attention_before_dense:
            attention = Attention()([encoding, encoding])
            global_average_encoding = GlobalAveragePooling1D()(encoding)
            global_average_attention = GlobalAveragePooling1D()(attention)
            x = Concatenate()([global_average_encoding, global_average_attention])
        else:
            x = GlobalAveragePooling1D()(encoding)
        # add FFN on top
        for i in range(len(self.num_dense)):
            cur_dense = self.num_dense[i]            
            x = layers.Dropout(0.1, name='dropout_' + str(i+1))(x)
            x = layers.Dense(cur_dense, activation="relu",name='dense_' + str(i+1))(x)       
        x = layers.Dropout(0.1, name='dropout_pre_final')(x)
        outputs = Dense(self.vocab_size_label, activation=self.last_activation,name='final_dense')(x)

        model = tf.keras.Model(inputs=[embedding_inputs, inputs_num], outputs=outputs)
        return model
        
    def get_model_architecture(self):
        return self.model.summary()

        
        
        
class LSTMLanguageModel(NNLanguageModel):
    def __init__(self, vocab_sizes, embed_dims, maxlen,
                 num_numerical,
                 lstm_units, name = 'lstm_language_model',
                 num_dense = [1024],
                 vocab_size_label = None,
                 flag_use_attention = True,
                 flag_use_attention_before_dense = True,
                 embed_shapes = None,
                 embedding_aggregation_mode = 'average',
                 last_activation = 'softmax'):
        super().__init__(name)
        
        self.name = name
        self.flag_use_attention = flag_use_attention
        self.vocab_sizes = vocab_sizes
        self.embed_dims = embed_dims
        self.num_numerical = num_numerical
        self.maxlen = maxlen
        self.lstm_units = lstm_units
        self.vocab_size_label = vocab_size_label
        if embed_shapes is None:
            self.embed_shapes = [1 for i in range(len(embed_dims))]
        else:
            self.embed_shapes = embed_shapes
        self.num_dense = num_dense
        self.embedding_aggregation_mode = embedding_aggregation_mode
        self.last_activation = last_activation
        self.flag_use_attention_before_dense = flag_use_attention_before_dense
        
        self.model = self.build_model()
        
        
    def build_model(self):
        if self.vocab_size_label is None:
            self.vocab_size_label = self.vocab_sizes[0]
    
        embedding_inputs = []
        embedding_list = []
        for i in range(len(self.embed_dims)):    
            if self.embed_shapes[i] == 1:
                embedding_inputs.append(layers.Input(shape=(self.maxlen,)))
                embedding_list.append(Embedding(input_dim = self.vocab_sizes[i], output_dim=self.embed_dims[i],input_length = self.maxlen)(embedding_inputs[-1]))
            else:
                embedding_inputs.append(layers.Input(shape=(self.maxlen,self.embed_shapes[i])))
                cur_embedding = Embedding(input_dim = self.vocab_sizes[i], output_dim=self.embed_dims[i],input_length = self.maxlen)
                cur_embedding_list = []
                for j in range(self.embed_shapes[i]):
                    cur_embedding_list.append(cur_embedding(embedding_inputs[-1][:,:,j]))
                average_embeddings = Average()(cur_embedding_list)
                embedding_list.append(average_embeddings)
        
        inputs_num = layers.Input(shape=(self.maxlen,self.num_numerical))
        if self.embedding_aggregation_mode == 'average':
            if len(embedding_list) > 1:
                before_embedding = Average()(embedding_list)
                before_embedding = Concatenate()([before_embedding, inputs_num])
            else:
                concat_input_list = list(embedding_list) + [inputs_num]
                before_embedding = Concatenate()(concat_input_list)
        elif self.embedding_aggregation_mode == 'concat':
            concat_input_list = list(embedding_list) + [inputs_num]
            
            # concatenate embedding and numerical features
            before_embedding = Concatenate()(concat_input_list)
        
        
                
        # add Attenton after Embedding
        if self.flag_use_attention:
            attention_seq = Attention()([before_embedding, before_embedding])
            before_embedding = Concatenate()([before_embedding, attention_seq])
        
            
        # add LSTM layers
        if type(self.lstm_units) == int:
            bidirect = Bidirectional(LSTM(self.lstm_units, return_sequences=False))(before_embedding)
        else:
            for i in range(len(self.lstm_units)-1):
                if i == 0:
                    bidirect = Bidirectional(LSTM(int(self.lstm_units[i]), return_sequences=True))(before_embedding)
                else:
                    bidirect = Bidirectional(LSTM(int(self.lstm_units[i]), return_sequences=True))(bidirect)
            bidirect = Bidirectional(LSTM(int(self.lstm_units[i]), return_sequences=False))(bidirect)
            
        # add Attention after CNN
        if self.flag_use_attention_before_dense:
            attention = Attention()([bidirect, bidirect])
            x = Concatenate()([bidirect, attention])
        else:
            x = bidirect
        # add FFN on top
        for i in range(len(self.num_dense)):
            cur_dense = self.num_dense[i]            
            x = layers.Dropout(0.1, name='dropout_' + str(i+1))(x)
            x = layers.Dense(cur_dense, activation="relu",name='dense_' + str(i+1))(x)       
        x = layers.Dropout(0.1, name='dropout_pre_final')(x)
        outputs = Dense(self.vocab_size_label, activation=self.last_activation,name='final_dense')(x)

        model = tf.keras.Model(inputs=[embedding_inputs, inputs_num], outputs=outputs)
        return model
        
    def get_model_architecture(self):
        return self.model.summary()


class TransformerLanguageModel(NNLanguageModel):
    def __init__(self, vocab_sizes, embed_dims, maxlen,
                 num_numerical,
                 num_heads, ff_dim,
                 num_blocks,
                 num_dense = [1024],
                 dropout_rate = 0.1,
                 embed_shapes = None,
                 vocab_size_label = None,
                 name = 'transformer_language_model',
                 flag_use_attention = True,
                 last_activation = 'softmax',
                 embedding_as_input = False):
        super().__init__(name)
        
        self.name = name
        self.flag_use_attention = flag_use_attention
        self.vocab_sizes = vocab_sizes
        self.embed_dims = embed_dims
        self.num_numerical = num_numerical
        self.maxlen = maxlen
        self.num_heads = num_heads
        self.num_blocks = num_blocks
        self.ff_dim = ff_dim
        self.vocab_size_label = vocab_size_label
        self.num_dense = num_dense
        self.dropout_rate = dropout_rate
        self.embedding_as_input = embedding_as_input
        if embed_shapes is None:
            self.embed_shapes = [1 for i in range(len(embed_dims))]
        else:
            self.embed_shapes = embed_shapes
        self.last_activation = last_activation
        
        self.model = self.build_model()
        
        
    def build_model(self):
        if self.embedding_as_input:
            embedding_input = layers.Input(shape=(self.maxlen,np.sum(self.embed_dims) + self.num_numerical), name='embedding_input_extern')
        else:
            embedding_inputs = []
            for i in range(len(self.embed_dims)):
                if self.embed_shapes[i] == 1:
                    embedding_inputs.append(layers.Input(shape=(self.maxlen,), name='embedding_input_' + str(i+1)))
                else:
                    embedding_inputs.append(layers.Input(shape=(self.maxlen,self.embed_shapes[i]), name='embedding_input_' + str(i+1)))
            inputs_num = layers.Input(shape=(self.maxlen,self.num_numerical), name='numerical_input')
            
            embedding_layer = TokenAndPositionEmbeddingMultipleInputs(maxlen = self.maxlen,
                                                              vocab_sizes = self.vocab_sizes,
                                                              embed_dims = self.embed_dims,
                                                              embed_shapes = self.embed_shapes,
                                                              num_numerical = self.num_numerical,
                                                              name='token_pos_embedding')
            
            if self.vocab_size_label is None:
                self.vocab_size_label = self.vocab_sizes[0]
                
            x = embedding_layer(embedding_inputs,inputs_num)
        attentions = []        
        for i in range(self.num_blocks):
            transformer_block = TransformerBlock(np.sum(self.embed_dims) + self.num_numerical, 
                            self.num_heads, self.ff_dim, name='transformer_block_' + str(i+1))
            if i == 0 and self.embedding_as_input:
                x, attention = transformer_block(embedding_input)
            else:
                x, attention = transformer_block(x)            
            attentions.append(attention)
        x = layers.GlobalAveragePooling1D(name='global_average')(x)
        for i in range(len(self.num_dense)):
            cur_dense = int(self.num_dense[i])
            x = layers.Dropout(0.1, name='dropout_' + str(i+1))(x)
            x = layers.Dense(cur_dense, activation="relu",name='dense_' + str(i+1))(x)       
        x = layers.Dropout(0.1, name='dropout_pre_final')(x)
        outputs = layers.Dense(self.vocab_size_label, activation=self.last_activation,name='final_dense')(x)
        
        if self.embedding_as_input:
            model = tf.keras.Model(inputs=embedding_input, outputs=outputs)
        else:
            model = tf.keras.Model(inputs=[embedding_inputs, inputs_num], outputs=outputs)
        return model
        
    
    def get_model_architecture(self):
        return self.model.summary()
        
